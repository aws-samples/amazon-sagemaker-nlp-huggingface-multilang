{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Transformers for 100+ Languages with HuggingFace on Amazon SageMaker: NLP Question Answering Mid-Resource Language Example\n",
    "\n",
    "In this Notebook, we experiment with different solution approaches using HuggingFace Transformers on Amazon SageMaker. We will fine tune various pretrained language models for Question & Answering task. Our focus will be on BERT variants.\n",
    "\n",
    "\n",
    "## Question Answering Task\n",
    "* Use Case\n",
    "  - _Extractive Question Answering_: \"Extractive QA odels are  deep learning models that can answer questions given some context.\n",
    "* Pretrained Language Models\n",
    "  1. _Multilingual-BERT_: “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, (2018). Links: [Paper](https://arxiv.org/pdf/1810.04805.pdf) - [Model](https://huggingface.co/bert-base-multilingual-uncased)\n",
    "  1. _BERTurk_: Turkish language model created by the MDZ Digital Library team (dbmdz) at the Bavarian State Library (Schweter 2020). Links: [Site](https://zenodo.org/record/3770924#.YgVaAvVBydY) - [Model](https://huggingface.co/dbmdz/bert-base-turkish-uncased)\n",
    "  1. _XLM-R_: A. Conneau et al., “Unsupervised Cross-Lingual Representation Learning at Scale”, (2019). Links: [Paper](https://arxiv.org/pdf/1911.02116.pdf) - [Model](https://huggingface.co/deepset/xlm-roberta-base-squad2)\n",
    "  \n",
    "* Dataset\n",
    "\n",
    "  - Turkish-Reading-Comprehension-Question-Answering-Dataset. Soygazi F., Çiftçi, Kök, Cengiz, THQuAD: \"Turkish Historic Question Answering Dataset for Reading Comprehension\", International Conference on Computer Science and Engineering (UBMK), 15-17 Sept. 2021. IEEE Article Link: https://ieeexplore.ieee.org/document/9559013 LICENSE: MIT Licence https://github.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset/blob/master/LICENSE\n",
    "\n",
    "## How to Run this Notebook\n",
    "\n",
    "You can run this notebook in SageMaker Studio. Please select the `PyTorch 1.6 Python 3.6 GPU Optimized` kernel.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "To train NLP models in SageMaker, you need to complete the following prerequisites:\n",
    "\n",
    "* Sign up for an AWS account. For more information, see [Set Up Amazon SageMaker Prerequisites](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).\n",
    "* Get started using  [Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning for QA Task - Example Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/nlp-finetune-qa-example.png](images/nlp-finetune-qa-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Environment Setup\n",
    "First we need to setup the Sagemaker API and retrieve the IAM role and the S3 bucket we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers \\\n",
    "             datasets \\\n",
    "            sentencepiece \\\n",
    "            ipywidgets \\\n",
    "            IProgress \\\n",
    "            sagemaker \\\n",
    "            sagemaker-experiments -qq -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM\n",
    "\n",
    "\n",
    "import time\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix_qa = 'datasets/hf-sagem-tr-qa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Prepare the Dataset, Explore Simple QA Task, Explore Tokenizers\n",
    "\n",
    "* Download the data\n",
    "* Prepare the data\n",
    "* Load as HuggingFace dataset\n",
    "* Explore the dataset with the HF Dataset Library\n",
    "* Demonstrate low-code QA task using HF Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset/master/data/2018-teknofest-squad-dataset/dev-v0.1.json -q\n",
    "!wget https://raw.githubusercontent.com/okanvk/Turkish-Reading-Comprehension-Question-Answering-Dataset/master/data/2018-teknofest-squad-dataset/train-v0.1.json -q\n",
    "\n",
    "!mkdir data\n",
    "\n",
    "!mv dev-v0.1.json data/dev-v0.1.json\n",
    "!mv train-v0.1.json data/train-v0.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def prep_dataset(input_filename, output_filename):\n",
    "    with open(input_filename) as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for article in dataset[\"data\"]:\n",
    "            title = article[\"title\"]\n",
    "            for paragraph in article[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                answers = {}\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    question = qa[\"question\"]\n",
    "                    idx      = qa[\"id\"]\n",
    "                    answers[\"text\"] = [str(a[\"text\"]) for a in qa[\"answers\"]]\n",
    "                    answers[\"answer_start\"] = [int(a[\"answer_start\"]) for a in qa[\"answers\"]]\n",
    "                    f.write(\n",
    "                        json.dumps(\n",
    "                            {\n",
    "                                \"id\": str(idx),\n",
    "                                \"title\": str(title),\n",
    "                                \"context\": str(context),\n",
    "                                \"question\": str(question),\n",
    "                                \"answers\": answers,\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_dataset('data/train-v0.1.json', 'data/train.json')\n",
    "prep_dataset('data/dev-v0.1.json', 'data/val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_file = sess.upload_data(\n",
    "    path=\"data/train.json\", bucket=sagemaker_session_bucket, key_prefix=s3_prefix_qa\n",
    ")\n",
    "\n",
    "test_input_file = sess.upload_data(\n",
    "    path=\"data/val.json\", bucket=sagemaker_session_bucket, key_prefix=s3_prefix_qa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = 'data/train.json'\n",
    "data_files[\"validation\"] = 'data/val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features in dataset: \\n Train = {}, \\n Validation = {}\".format(len(ds['train']), len(ds['validation'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(ds['train']).iloc[3815])\n",
    "\n",
    "# * English Translation:\n",
    "\n",
    "# * title: Emre Kongar\n",
    "# * context: Resit Emre Kongar (b. 13 October 1941, Istanbul), Turkish sociologist, professor. \n",
    "# * question: What is the academic title of Emre Kongar?\n",
    "# * answers: {'text': ['professor'], 'answer_start': [68]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] A low-code QA task using HF Pipelines\n",
    "\n",
    "Please note that if an existing pretrained model exists in the HF Hub, which is sufficient for your scenario, and no fine-tuning is required, you could use HF Pipelines and simply provide the task name (e.g. \"Question and Answering\"), and the name of the model as follows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * English Translation:\n",
    "\n",
    "# * question: What is the most popular animal in Istanbul?\n",
    "# * context: Istanbul is famous for its history, the many empires that have existed here, its majestic skyline and the stunning Bosphorus. \n",
    "#     But, actually, this is the city of cats. Istanbulites share the city with approximately 125,000 cats. \n",
    "#     And these are just the street cats; when house cats are included, the feline population rises to about 200,000. \n",
    "#     More than the population of many cities and countries! \n",
    "#     We can illustrate Istanbul’s love for cats with a quote from Ernest Hemingway:\n",
    "#     A cat has absolute emotional honesty: human beings, for one reason or another, may hide their feelings, but a cat does not.\n",
    "# # answers: top 1: \"from cats\", top 2: \"street cats\", top 3: \"its cats\"\n",
    "model_name = \"deepset/xlm-roberta-base-squad2\"\n",
    "\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "context = 'İstanbul tarihi, burada var olan birçok imparatorluk, görkemli silüeti ve çarpıcı Boğaz ile ünlüdür. \\\n",
    "Ama aslında burası kedilerin şehri. İstanbullular şehri yaklaşık 125,000 kediyle paylaşıyor. \\\n",
    "Ve bunlar sadece sokak kedileri; ev kedileri dahil edildiğinde kedi nüfusu yaklaşık 200,000e yükselir. \\\n",
    "Birçok şehir ve ülkenin nüfusundan daha fazla! İstanbul\\'un kedilere olan sevgisini \\\n",
    "Ernest Hemingway\\'den bir alıntıyla gösterebiliriz: Bir kedinin mutlak duygusal dürüstlüğü vardır: \\\n",
    "insanlar, bir sebepten ötürü duygularını gizleyebilir, ancak bir kedi saklamaz.'\n",
    "\n",
    "question = 'İstanbulun en popüler hayvanı hangisidir?'\n",
    "\n",
    "\n",
    "pd.DataFrame(nlp(question=question, context=context, top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Experiment Fine-Tuning Strategies for Question-Answering Task in Your Language \n",
    "\n",
    "For our example we will take the following pre-trained Turkish models available via HuggingFace Hub: \n",
    "\n",
    "* BERTurk \n",
    "* Multilingual BERT \n",
    "* XLM-RoBERTa Base \n",
    "\n",
    "and fine tune on two different SageMaker instance types (and GPU architectures):\n",
    "\n",
    "* p2.2xlarge  (GPU: Nvidia  K80 GPU. GPU Architecture: Kepler (2012)\n",
    "* p3.2xlarge  (GPU: Nvidia V100 GPU. GPU Architecture: Volta  (2017)\n",
    "* g4dn.xlarge (GPU: Nvidia   T4 GPU. GPU Architecture: Turing (2018)\n",
    "\n",
    "[SageMaker Training Instance Pricing](https://aws.amazon.com/sagemaker/pricing/) \n",
    "\n",
    "We will use SageMaker Experiments to track our model training experiments and compare the performance of these models.\n",
    "\n",
    "\n",
    "Once the Experiment is created, we can keep track of metadata such as \n",
    "\n",
    "- hyperparameters\n",
    "- artifacts (input data, output model location) or \n",
    "- create custom metadata. In our example, we will add static metadata such as model parameter size & number of layers, and dynamic metadata such as custom metrics and training duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model list & metric definitions\n",
    "Now we can create a list of models we want to compare for the Turkish QA task. In terms of costs, please note that a separate training job for each model will be launched. We then define the metrics we want to use for comparing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_configs = []\n",
    "\n",
    "config = {}\n",
    "config['model'] = 'dbmdz/bert-base-turkish-uncased'\n",
    "config['instance_type'] = 'ml.g4dn.xlarge'\n",
    "config['instance_count'] = 1\n",
    "config['distribution'] = None\n",
    "trial_configs.append(config)\n",
    "\n",
    "config = {}\n",
    "config['model'] = 'bert-base-multilingual-uncased'\n",
    "config['instance_type'] = 'ml.p3.2xlarge'\n",
    "config['instance_count'] = 1\n",
    "config['distribution'] = None\n",
    "trial_configs.append(config)\n",
    "\n",
    "config = {}\n",
    "config['model'] = 'deepset/xlm-roberta-base-squad2'\n",
    "config['instance_type'] = 'ml.g4dn.xlarge'\n",
    "config['instance_count'] = 1\n",
    "config['distribution'] = None\n",
    "trial_configs.append(config)\n",
    "\n",
    "config = {}\n",
    "config['model'] = 'deepset/xlm-roberta-base-squad2'\n",
    "config['instance_type'] = 'ml.p3.2xlarge'\n",
    "config['instance_count'] = 1\n",
    "config['distribution'] = None\n",
    "trial_configs.append(config)\n",
    "\n",
    "config = {}\n",
    "config['model'] = 'deepset/xlm-roberta-base-squad2'\n",
    "config['instance_type'] = 'ml.p3.16xlarge'\n",
    "config['instance_count'] = 2\n",
    "# Define the distribution parameters in the HuggingFace Estimator\n",
    "config['distribution'] = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "trial_configs.append(config)\n",
    "\n",
    "trial_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Management using SageMaker Experiments\n",
    "\n",
    "Now we create the experiment. We first choose a unique name and then use the SM Experiemnt API to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "experiment_name = f\"NLP-QuestionAnswer-HF-fine-tune-tr-{create_date}\"\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_experiment = Experiment.create(\n",
    "    experiment_name=experiment_name,\n",
    "    description=\"Fine-tune Transformers for Q-A Task with HuggingFace on Amazon SageMaker\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managed Training using HF Deep Learning Containers on Amazon SageMaker\n",
    "\n",
    "Setting up, managing, and scaling custom ML environments can be time-consuming and cumbersome even for experts. With AWS Deep Learning Containers (DLC), you get access to prepackaged and optimized DL frameworks that make it easy for you to customize, extend, and scale your environments.\n",
    "\n",
    "To train HuggingFace models on SageMaker, we will use the HuggingFace Estimator available in the SageMaker Python SDK, which makes it easy to fine-tune and customise models from HuggingFace Hub. We instantiate this class with the following inputs:\n",
    "\n",
    "* a training script which uses the Transformers library to prepare batches of data for training, then uses PyTorch to train our model on SageMaker-managed training instances. In our example, we provide a script called run_qa.py (available from transformers library).\n",
    "* training configuration such as model name, and hyperparameters such as batch size, number of epochs, etc. \n",
    "* SageMaker instance type and count\n",
    "* Whether to use Spot Instances for training\n",
    "* Two popular custom metric definitions for Question Answering task: F1 score and Exact Match (EM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify a training script that is stored in a GitHub repository as the entry point for our Estimator, \n",
    "# so we don’t have to download the scripts locally.\n",
    "# git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.10.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in trial_configs:\n",
    "    model = config['model'].split('/')[-1]\n",
    "    instance = config['instance_type'].replace('.', '-')\n",
    "    trial_name = f\"NLP-QA-Trial-{model}-{instance}-{int(time.time())}\"\n",
    "    \n",
    "    # create a trial that will be attached to the experiment\n",
    "    nlp_trial = Trial.create(\n",
    "        trial_name=trial_name,\n",
    "        experiment_name=nlp_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm,\n",
    "    )\n",
    "\n",
    "    hyperparameters_qa={\n",
    "        'model_name_or_path': config['model'],\n",
    "        'train_file': '/opt/ml/input/data/train/train.json',\n",
    "        'validation_file': '/opt/ml/input/data/val/val.json',\n",
    "        'do_train': True,\n",
    "        'do_eval': True,\n",
    "        'fp16': True,\n",
    "        'per_device_train_batch_size': 16,\n",
    "        'per_device_eval_batch_size': 16,\n",
    "        'num_train_epochs': 2,\n",
    "        'max_seq_length': 384,\n",
    "        'pad_to_max_length': True,\n",
    "        'doc_stride': 128,\n",
    "        'output_dir': '/opt/ml/model'\n",
    "    }\n",
    "\n",
    "    huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                        source_dir='./sagemaker',\n",
    "                                        # source_dir='./examples/pytorch/question-answering',\n",
    "                                        # git_config=git_config,\n",
    "                                        instance_type=config['instance_type'],\n",
    "                                        instance_count=config['instance_count'],\n",
    "                                        role=role,\n",
    "                                        transformers_version='4.12.3',\n",
    "                                        pytorch_version='1.9.1',\n",
    "                                        py_version='py38',\n",
    "                                        distribution=config['distribution'],\n",
    "                                        hyperparameters=hyperparameters_qa,\n",
    "                                        metric_definitions=metric_definitions,\n",
    "                                        enable_sagemaker_metrics=True,)\n",
    "    \n",
    "    nlp_training_job_name = f\"NLPjob-{model}-{instance}-{int(time.time())}\"\n",
    "    \n",
    "    training_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_qa}/'\n",
    "    test_input_path = f's3://{sagemaker_session_bucket}/{s3_prefix_qa}/'\n",
    "    \n",
    "    huggingface_estimator.fit(\n",
    "        inputs={'train': training_input_path, 'val': test_input_path},\n",
    "        job_name=nlp_training_job_name,\n",
    "        experiment_config={\n",
    "            \"ExperimentName\": nlp_experiment.experiment_name,\n",
    "            \"TrialName\": nlp_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": nlp_trial.trial_name,},\n",
    "        wait=False,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Launched Trial for: \\n Model = {}, \\n Instance Type = {}\".format(model, instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Compare Results\n",
    "\n",
    "Once all training jobs have completed we can retrieve various metadata via the SM Experiment API and compare the performance of our models. The `ExperimentAnalytics` object available from the SageMaker Experiments library contains many info such as the custom metric definitions (F1 and EM - Exact Match)\n",
    "\n",
    "We will enrich this metadata with custom info such as \n",
    "\n",
    "* training duration information provided by the SageMaker Training Job\n",
    "* info from HF model config related to model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Metadata from each Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    experiment_name=nlp_experiment.experiment_name\n",
    ")\n",
    "\n",
    "df_results = trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_jobs = list(df_results['sagemaker_job_name'])\n",
    "\n",
    "for job in training_jobs:\n",
    "    description = sess.describe_training_job(job.replace('\"', ''))\n",
    "    df_results.loc[df_results['sagemaker_job_name'] == job, 'TrainingTimeInMinutes'] = description['TrainingTimeInSeconds'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Model Capacity Related Infro from HF model config\n",
    "We can retrieve the configdetails from the config.json file in the model's repo (e.g. https://huggingface.co/dbmdz/bert-base-turkish-uncased/raw/main/config.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in trial_configs:\n",
    "    r = requests.get(f\"https://huggingface.co/{config['model']}/raw/main/config.json\")\n",
    "    model_config = json.loads(r.text)\n",
    "    df_results.loc[df_results['model_name_or_path'].str.replace('\"', '') == config['model'], 'vocab_size'] = model_config['vocab_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Costs (in USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_prices(region, purpose):\n",
    "    endpoint = \"https://b0.p.awsstatic.com/pricing/2.0/meteredUnitMaps/sagemaker/USD/current/sagemaker-instances.json\"\n",
    "    response = requests.get(endpoint).json()\n",
    "    price_dict = {}\n",
    "\n",
    "    for instance_region, region_data in response[\"regions\"].items():\n",
    "        if instance_region == region:\n",
    "            for instance_type, instance_data in region_data.items():\n",
    "                instance_purpose = instance_data[\"Instance Type\"].split('-')[-1]\n",
    "                if instance_purpose == purpose:\n",
    "                    price_dict[instance_data[\"Instance\"]] = instance_data[\"price\"]\n",
    "                    \n",
    "    return price_dict\n",
    "\n",
    "regions = pd.read_csv('regions.csv')\n",
    "region_name = regions.loc[regions['region_code'] == sess.boto_region_name]['region_name'].item()\n",
    "\n",
    "prices = get_instance_prices(region=region_name, purpose=\"Training\")\n",
    "\n",
    "df_results['cost_per_hour_per_instance_usd'] = df_results.apply(lambda x: float(prices[x['SageMaker.InstanceType']]), axis=1)\n",
    "df_results['training_job_cost_usd'] = df_results['TrainingTimeInMinutes'] * 60 * df_results['cost_per_hour_per_instance_usd'] * df_results['SageMaker.InstanceCount'] / 3600.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[['Experiments', 'model_name_or_path', 'vocab_size', 'SageMaker.InstanceType', 'SageMaker.InstanceCount', 'sagemaker_distributed_dataparallel_enabled', 'num_train_epochs', \\\n",
    "    'exact_match - Avg', 'f1 - Avg', 'TrainingTimeInMinutes', 'cost_per_hour_per_instance_usd', 'training_job_cost_usd']]  \\\n",
    "    .sort_values(by=['exact_match - Avg', 'f1 - Avg'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(f\"sm-experiment-results-{create_date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also review results from SageMaker Studio UI\n",
    "\n",
    "\n",
    "Experiment Results Comparison Table:\n",
    "\n",
    "![images/SMExperiment-results-table.png](images/SMExperiment-results-table.png)\n",
    "\n",
    "\n",
    "Evaluation Metric Results Comparison Graph:\n",
    "\n",
    "![images/SMExperiment-results-graph.png](images/SMExperiment-results-graph.png)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
